_Now serving..._

图像优化

=================

Written by [Jordan Morgan](https://www.twitter.com/jordanmorgan10) • Dec 11th, 2018

有人说，最好的相机就是你总是随身携带的那个相机。如果这个谚语还算有些分量的话，那么毫无疑问的 - iPhone 彻头彻尾的就是这个行星上最重要的相机。我们的工业状况也证实了这一点。

在度假？如果没有在 Instagram 上发布一些你的随性拍照的话，那么这件事就好像没有发生过一样。

突发新闻？赶紧刷刷推特看看事件的实时展开的图片，以此了解媒体的报道内容。

等等。

但是，相对于它们在平台上的无处不在，以高性能和节省内存的方式来展示它们的行为很容易变成管理不善的努力。 稍微了解 UIKit 内部发生的事情，以及这样处理图像的原因，你就可以获得巨大的资源节约，并从对 Jetsam[1] 的无情愤怒中解脱出来。

### 来一点理论知识

突击测试 - 如下我的美丽的女儿的这张 266K 大小的照片在 iOS 应用中需要多少内存？

![Baylor](../assets/images/baylor.jpg)

高能预警 - 需要的不是 266K 内存。也不是 2.66M 内存。实际上，大约是 14M 内存。

为什么？

iOS 基本上从图片的几何尺寸推断内存占用 - 和实际的文件大小关系不大。这张照片的几何尺寸是 1718像素点宽度乘以 2048 像素点的宽度。假设一个像素点需要 4 个字节，那么：

    1718 * 2048 * 4 / 1024 / 1024 = 13.42M
    
假定你有一个 TableView 组件，显示一个用户列表，在每一行的左侧显示一个无处不在的照片构成的圆形头像。如果你认为事情进行的应该不错，因为你已经通过 ImageOptim 或者别的什么类似的软件对照片做过压缩处理，那么我要说的是，事实并非你认为的那样。即使每一张照片都是比较节省的 256X256 幅面，你依然还是占用了太多的内存。


### 渲染管道线

这就是说 - 了解底层在进行的情况是值得的。当你加载一张图片，它会被进行 3 步处理：

1） 加载。 iOS 获得压缩图片（在我们的案例内）加载 266K 到内存。到这里还不必担心什么。
2） 解码。 现在 iOS 拿到这个图片把它转换为 GPU 可以解读的格式。现在图片被解压了，就在这里，我们需要 14MB 的内存了。
3） 渲染。 正如听到的那样，图片数据已经准备好可以任何方式去渲染。即使只是一个 60X60 的图片视图。

解码阶段是一个内存热点。这里 iOS 创建了一个缓冲区 - 再具体的说是图片缓冲区，它是一个内存中的图片的表达。所以，内存占用的大小本质上和图片自身的几何尺寸而不是文件大小相关联。这清楚地描绘了为什么几何尺寸在计算处理图像时的内存消耗时是如此的重要。

特别对于 `UIImage` , 当我们从网络或者别的什么来源来接收到图片的时候，它会把数据缓冲区按照数据指定的压缩格式来解码，(考虑到PNG 或者 JPEG)。 然后就会把它保持在那里. 因为渲染并不是一个一次性操作, 因此`UIImage`保留了图像缓冲区，这样它只需要解码一次。

扩展这个想法 - 任何iOS应用程序的一个完整缓冲区是它的帧缓冲区，它负责实际显示您的iOS应用程序，帧缓冲区显示在屏幕上，因为它保存了其内容的渲染输出。 任何iOS设备上的显示硬件都使用以像素为单位组织的信息来点亮物理屏幕上的像素。

时间在这里很重要。为了获得如同黄油一般顺滑的每秒60帧的滚动，当窗口信息变化时（比如把图片指定给图片视图），你将需要 UIKit 渲染应用程序的窗口和它的子视图到帧缓冲器内。如果你做得慢了，你就丢帧了。

> 想想1/60秒的时间完成一帧还不够？ `Pro Motion` 设备把赌注提高到1/120秒。

### 显示幅面非常关键

我们可以把这个过程和内存占用做一个漂亮的可视化。使用我的女儿的图片，我创建了一个平凡的应用，它使用 ImageView 显示此图片：


    let filePath = Bundle.main.path(forResource:"baylor", ofType: "jpg")!
    let url = NSURL(fileURLWithPath: filePath)
    let fileImage = UIImage(contentsOfFile: filePath)
    
    // Image view
    let imageView = UIImageView(image: fileImage)
    imageView.translatesAutoresizingMaskIntoConstraints = false
    imageView.contentMode = .scaleAspectFit
    imageView.widthAnchor.constraint(equalToConstant: 300).isActive = true
    imageView.heightAnchor.constraint(equalToConstant: 400).isActive = true
    
    view.addSubview(imageView)
    imageView.centerXAnchor.constraint(equalTo: view.centerXAnchor).isActive = true
    imageView.centerYAnchor.constraint(equalTo: view.centerYAnchor).isActive = true
    
> 我们这里使用的是一个简单的案例场景，请自行脑补在生产环境中的内存占用的规模。

快速的查看下 LLDB 我们可以获知，这个工作中的图片的几何尺寸大小，即使我们使用的ImageView其实是比较小尺寸的(300 X 400)。

    <UIImage: 0x600003d41a40>, {1718, 2048}
    
提请注意 - 它的单位是`点`。因此如果我在一个 3x 或者 2x 的设备上，那么你可能需要把这个数字乘以对应的倍率。现在我们快速的看看`vmmap` 确认下这个图是否真实占用了 14M 内存。

    vmmap --summary baylor.memgraph

几个数据需要拿出来看（为了简短期间，做了部分删减）：

    Physical footprint:         69.5M
    Physical footprint (peak):  69.7M
    
我们现在发现当前使用差不多 70M 。这个给我们一个重构的基线。如果我们通过`Image IO`做一次筛选，我们可以看到图片的成本：

    vmmap --summary baylor.memgraph | grep "Image IO"
    
    Image IO  13.4M   13.4M   13.4M    0K  0K  0K   0K  2 
    
哦 - 现在这里有大约 14M 的脏内存。那是我们通过餐巾纸上潦草的计算假设我们的图片会占用的内存。为了提供一个上下文，我们可以看大一个快速的终端屏幕截图，它加上了每一个列的标题，以便更容易看懂每一个列的含义：

![Baylor](../assets/images/vmmap.jpg)

非常明显，我们为一个只有 300 X 400 的图片支付了完整的图片成本。图片的大小是一个关键，但是它也不是需要注意的唯一。

### 色彩空间

您要求的部分内存取决于另一个重要因素 - 色彩空间。 在上面的例子中，我们假设的情况并不是大多数 iPhone 应用的情况 - 图像使用的是sRGB格式。 通过给出红色，蓝色，绿色和alpha分量的各一个字节，为每个像素占用4个字节。

如果您使用支持广泛颜色格式的设备（例如，iPhone 8+或iPhone X）进行拍摄，那么您可能会全面的实现翻倍。 当然，反过来也是如此。 [Metal](https://en.wikipedia.org/wiki/Metal_(API)) 可以使用Alpha 8格式，其名称只有一个通道[TODO]。

这就产生了不少考量和管理的事项了。这就是为什么你应该使用 [UIGraphicsImageRenderer](https://swiftjectivec.com/UIGraphicsImageRenderer)而不是使用`UIGraphicsBeginImageContextWithOptions`。后者总是使用 sRGB ，这意味着你可以失去你本来想要的宽格式，或者失去本来可以节省的空间。到了 iOS 12 版本，`UIGraphicsImageRenderer` 会帮你选择一个正确的。

有必要做一个提醒 - 很多图像并不是真正的拍摄出来的，而只是一些琐碎的绘画操作。 本来不想把我最近写的内容再老调重弹，但万一你错过了它：

    let circleSize = CGSize(width: 60, height: 60)    
    UIGraphicsBeginImageContextWithOptions(circleSize, true, 0)    
    // Draw a circle
    let ctx = UIGraphicsGetCurrentContext()!
    UIColor.red.setFill()
    ctx.setFillColor(UIColor.red.cgColor)
    ctx.addEllipse(in: CGRect(x: 0, y: 0, width: circleSize.width, height: circleSize.height))
    ctx.drawPath(using: .fill)
    let circleImage = UIGraphicsGetImageFromCurrentImageContext()
    UIGraphicsEndImageContext()
    
上面的圆形图像使用每像素 4 字节格式。 如果你使用`UIGraphicsImageRenderer`来处理它，你可以通过让渲染器自动选择正确的格式，每个像素给予 1 个字节，从而可以节省高达 75％ 的内存成本：

    let circleSize = CGSize(width: 60, height: 60)
    let renderer = UIGraphicsImageRenderer(bounds: CGRect(x: 0, y: 0, width: circleSize.width, height: circleSize.height))    
    let circleImage = renderer.image{ ctx in
        UIColor.red.setFill()
        ctx.cgContext.setFillColor(UIColor.red.cgColor)
        ctx.cgContext.addEllipse(in: CGRect(x: 0, y: 0, width: circleSize.width, height: circleSize.height))
        ctx.cgContext.drawPath(using: .fill)
    }
    

### 比例降级或者采样降级

超越简单的绘图场景 - 图像的许多问题及其对内存占用的影响源于我们与实际摄影艺术相关的典型照片。 想想肖像，风景照片等等。

按理说，一些工程师可能会假设（并且逻辑上如此）只需通过`UIImage`缩减它们就足够了。 但它通常不会由于上述原因，并且根据Apple的Kyle Howarth，由于内部坐标空间转换，它也没有那么高效。

“UIImage”在这里成为一个问题，主要是因为它会将原始图像解压缩到内存中，正如我们在查看渲染管道时所讨论的那样。 理想情况下，我们需要一种方法来减小图像缓冲区的大小。

值得庆幸的是，可以仅以实际调整大小的图像为代价来调整图像大小，这可能是人们会认为正在进行的，但是实际上并不会如此。让我们尝试下载到更低级别的API，然后对其进行下采样：

    let imageSource = CGImageSourceCreateWithURL(url, nil)!
    let options: [NSString:Any] = [kCGImageSourceThumbnailMaxPixelSize:400,
                                   kCGImageSourceCreateThumbnailFromImageAlways:true]
    
    if let scaledImage = CGImageSourceCreateThumbnailAtIndex(imageSource, 0, options as CFDictionary) {
        let imageView = UIImageView(image: UIImage(cgImage: scaledImage))
        
        imageView.translatesAutoresizingMaskIntoConstraints = false
        imageView.contentMode = .scaleAspectFit
        imageView.widthAnchor.constraint(equalToConstant: 300).isActive = true
        imageView.heightAnchor.constraint(equalToConstant: 400).isActive = true
        
        view.addSubview(imageView)
        imageView.centerXAnchor.constraint(equalTo: view.centerXAnchor).isActive = true
        imageView.centerYAnchor.constraint(equalTo: view.centerYAnchor).isActive = true
    }
    
从视觉的角度看，我们得到了与以前完全相同的结果。 但是在这里，我们已经把图像通过`CGImageSourceCreateThumbnailAtIndex（）`做了处理，而不是仅仅将未经缩减的图像放入图像视图中。真理再次来源于“vmmap”命令的执行结果中。一起来看看我们的优化是否得到了回报（同样的，为简洁而做了剪辑）：

    vmmap -summary baylor Optimized.memgraph
    
    Physical footprint:         56.3M
    Physical footprint (peak):  56.7M
    
内存已经开始节约了。如果我们比较之前的 69.5M 到现在的 56.3M ，我们可以节省 13.2M。 这是一个巨大的节约，几乎等于整个图像文件本身的大小。

此外，您可以使用许多选项来磨砺你的这个案例，以满足您的使用需求。 在WWDC 18 的第 219 节“图像和图形最佳实践”中，Apple 工程师 Kyle Sluder 展示了一种有趣的方法，通过`kCGImageSourceShouldCacheImmediately`标志来控制解码的过程：

    func downsampleImage(at URL:NSURL, maxSize:Float) -> UIImage
    {
        let sourceOptions = [kCGImageSourceShouldCache:false] as CFDictionary
        let source = CGImageSourceCreateWithURL(URL as CFURL, sourceOptions)!
        let downsampleOptions = [kCGImageSourceCreateThumbnailFromImageAlways:true,
                                 kCGImageSourceThumbnailMaxPixelSize:maxSize
                                 kCGImageSourceShouldCacheImmediately:true,
                                 kCGImageSourceCreateThumbnailWithTransform:true,
                                 ] as CFDictionary
        
        let downsampledImage = CGImageSourceCreateThumbnailAtIndex(source, 0, downsampleOptions)!
        
        return UIImage(cgImage: downsampledImage)
    }
    

在这个案例中，在您特别要求缩略图之前，Core Graphics 不会费心解码图像。 另外，请注意记得不要忘了传递`kCGImageSourceCreateThumbnailMaxPixelSize`标志进来正如我们在两个示例中所做的那样。因为如果不这样做 - 您将获得与图像大小相同的缩略图。 来自文档：

>“...如果未指定最大像素大小，则缩略图将是完整图像的大小，这可能不是您想要的。”

所以发生了什么事？ 简而言之，我们通过将缩小图片采样后放入缩略图中来创建比以前小得多的解码图像缓冲区。 回想一下我们的渲染管道，对于第一部分（负载），我们建立了一个图像缓冲区，以便“UIImage”解码。它只表示我们正在显示的图像视图的大小，而不是反映整个图像的实际尺寸。

TODO想要TL;DR 这整篇文章 ？寻找机会对图像进行缩减采样，而不是使用“UIImage”来缩减尺寸。

### 额外奖励

我个人会把以上方法和iOS 11中引入的 [prefetch API](https://developer.apple.com/documentation/uikit/uitableviewdatasourceprefetching?language=swift) 结合起来使用。请记住，在我们解码时，我们肯定会会引入CPU使用率的高峰 ，即使在表或集合视图需要我们的单元格之前执行它也是如此。

由于iOS在连续需要电力时能够有效地管理电力需求，而在我们的这种情况下，它很可能是断断续续的电力需求，所以使用你自己的队列以解决这样的问题是很好的。 这样做也会把解码过程移动到后台进程，这是另一个重要的好处。

TODO遮住你的眼睛，以下来自我业余项目中的 Objective-C 代码示例：

    // Use your own queue instead of a global async one to avoid potential thread explosion爆炸
    - (void)tableView:(UITableView *)tableView prefetchRowsAtIndexPaths:(NSArray<NSIndexPath *> *)indexPaths
    {
        if (self.downsampledImage != nil || 
            self.listItem.mediaAssetData == nil) return;
        
        NSIndexPath *mediaIndexPath = [NSIndexPath indexPathForRow:0
                                                         inSection:SECTION_MEDIA];
        if ([indexPaths containsObject:mediaIndexPath])
        {
            CGFloat scale = tableView.traitCollection.displayScale;
            CGFloat maxPixelSize = (tableView.width - SSSpacingJumboMargin) * scale;
            
            dispatch_async(self.downsampleQueue, ^{
                // Downsample
                self.downsampledImage = [UIImage downsampledImageFromData:self.listItem.mediaAssetData
                                   scale:scale
                            maxPixelSize:maxPixelSize];
                
                dispatch_async(dispatch_get_main_queue(), ^ {
                    self.listItem.downsampledMediaImage = self.downsampledImage;
                });
            });
        }
    }
    

TODO>请注意将资产目录用于原始图像资产的大部分，因为它已经为您管理缓冲区大小（以及更多）。

有关如何成为所有事物记忆和图像的一等公民的更多灵感，请务必从WWDC 18中捕获这些特别丰富的信息：


*   [iOS Memory Deep Dive](https://developer.apple.com/videos/play/wwdc2018/416/?time=1074)
*   [Images and Graphics Best Practices](https://developer.apple.com/videos/play/wwdc2018/219/)

### Wrapping Up

你无法获知你不知道什么。 在编程方面，你基本上把自己注册到了一个每小时都需要跑 10,000 英里的职业生涯中，以便跟上创新和变化的步伐。 这意味着......你将会发现一大堆你根本不知道的API，框架，模式或优化。

这一点在图像处理方面肯定是正确的。 大多数情况下，您使用一些漂亮的像素初始化一个“UIImageView”并继续前进。 我明白，摩尔定律等等。 这些手机速度很快，并且有很多内存，嘿 - 我们用一台内存不到 100K 字节的计算机将人类送到了月球上呢。

但是与魔鬼一起跳舞的时间足够长，他一定会把他的角养大的。 不要让jetsam从存在中解雇你，因为自拍需要1 GB的内存。 希望这些知识和这些技术可以帮助您减少查阅崩溃日志的次数。

下一次见✌️。

1. Jetsam。是一个监视OSX和iOS中内存使用的系统。它保留了设备上的进程列表，用于监视设备耗尽空闲RAM的情况，并寻找可以释放RAM的内容
